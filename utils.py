import streamlit as st
import duckdb
import pandas as pd
import google.generativeai as genai
import geopandas as gpd
from typing import Optional
import logging
import re
import json
import os

logger = logging.getLogger(__name__)

# --- å®šæ•°å®šç¾© ---
TABLE_SCHEMA = """
CREATE TABLE business_stats("year" INTEGER, town_name VARCHAR, industry_name VARCHAR, num_offices INTEGER, num_employees INTEGER);
CREATE TABLE population("year" BIGINT, town_name VARCHAR, num_households BIGINT, num_population BIGINT, num_male BIGINT, num_female BIGINT);
CREATE TABLE crimes("year" BIGINT, town_name VARCHAR, major_crime VARCHAR, minor_crime VARCHAR, crime_count BIGINT);
"""

COLUMN_DEFINITIONS = """
year: å¹´åº¦
town_name: ç”ºå
industry_name: äº‹æ¥­ç¨®åˆ¥
num_offices: äº‹æ¥­æ‰€æ•°
num_employees: å¾“æ¥­è€…æ•°
num_households: ä¸–å¸¯æ•°
num_population: äººå£æ•°
num_male: ç”·æ€§æ•°
num_female: å¥³æ€§æ•°
major_crime: çŠ¯ç½ªå¤§åˆ†é¡
minor_crime: çŠ¯ç½ªå°åˆ†é¡
crime_count: çŠ¯ç½ªä»¶æ•°
"""

INDUSTRY_NAMES = """
è¾²æ—æ¼æ¥­, é‰±æ¥­_æ¡çŸ³æ¥­_ç ‚åˆ©æ¡å–æ¥­, å»ºè¨­æ¥­, è£½é€ æ¥­, é›»æ°—ï½¥ã‚¬ã‚¹ï½¥ç†±ä¾›çµ¦ï½¥æ°´é“æ¥­, 
æƒ…å ±é€šä¿¡æ¥­, é‹è¼¸æ¥­_éƒµä¾¿æ¥­, å¸å£²æ¥­_å°å£²æ¥­, é‡‘èæ¥­_ä¿é™ºæ¥­, ä¸å‹•ç”£æ¥­_ç‰©å“è³ƒè²¸æ¥­, 
å­¦è¡“ç ”ç©¶_å°‚é–€ï½¥æŠ€è¡“ã‚µãƒ¼ãƒ“ã‚¹æ¥­, å®¿æ³Šæ¥­_é£²é£Ÿã‚µãƒ¼ãƒ“ã‚¹æ¥­, ç”Ÿæ´»é–¢é€£ã‚µãƒ¼ãƒ“ã‚¹æ¥­_å¨¯æ¥½æ¥­, 
æ•™è‚²_å­¦ç¿’æ”¯æ´æ¥­, åŒ»ç™‚_ç¦ç¥‰, è¤‡åˆã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­, ã‚µãƒ¼ãƒ“ã‚¹æ¥­ï¼ˆä»–ã«åˆ†é¡ã•ã‚Œãªã„ã‚‚ã®ï¼‰, 
å…¬å‹™ï¼ˆä»–ã«åˆ†é¡ã•ã‚Œã‚‹ã‚‚ã®ã‚’é™¤ãï¼‰
"""

CRIMES_TYPES = """
å‡¶æ‚ªçŠ¯:å¼·ç›—,
å‡¶æ‚ªçŠ¯:ãã®ä»–,
ç²—æš´çŠ¯:å‚·å®³,
ç²—æš´çŠ¯:æå–,
ç²—æš´çŠ¯:æš´è¡Œ,
ç²—æš´çŠ¯:è„…è¿«,
ä¾µå…¥çªƒç›—:äº‹å‹™æ‰€è’ã—,
ä¾µå…¥çªƒç›—:å‡ºåº—è’ã—,
ä¾µå…¥çªƒç›—:å­¦æ ¡è’ã—,
ä¾µå…¥çªƒç›—:å±…ç©ºã,
ä¾µå…¥çªƒç›—:å¿è¾¼ã¿,
ä¾µå…¥çªƒç›—:ç©ºãå·£,
ä¾µå…¥çªƒç›—:é‡‘åº«ç ´ã‚Š,
ä¾µå…¥çªƒç›—:ãã®ä»–,
éä¾µå…¥çªƒç›—:ã™ã‚Š,
éä¾µå…¥çªƒç›—:ã²ã£ãŸãã‚Š,
éä¾µå…¥çªƒç›—:ã‚ªãƒ¼ãƒˆãƒã‚¤ç›—,
éä¾µå…¥çªƒç›—:ä¸‡å¼•ã,
éä¾µå…¥çªƒç›—:å·¥äº‹å ´ã­ã‚‰ã„,
éä¾µå…¥çªƒç›—:ç½®å¼•ã,
éä¾µå…¥çªƒç›—:è‡ªå‹•è»Šç›—,
éä¾µå…¥çªƒç›—:è‡ªè²©æ©Ÿã­ã‚‰ã„,
éä¾µå…¥çªƒç›—:è‡ªè»¢è»Šç›—,
éä¾µå…¥çªƒç›—:è»Šä¸Šã­ã‚‰ã„,
éä¾µå…¥çªƒç›—:ãã®ä»–,
ãã®ä»–:å æœ‰é›¢è„±ç‰©æ¨ªé ˜,
ãã®ä»–:è©æ¬º,
ãã®ä»–:è³­åš,
ãã®ä»–:ãã®ä»–åˆ‘æ³•çŠ¯,
ãã®ä»–:ãã®ä»–çŸ¥èƒ½çŠ¯,
"""

PROMPT_TEMPLATE = f"""
ã‚ãªãŸã¯å„ªç§€ãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚å…«ç‹å­å¸‚ã«é–¢ã™ã‚‹ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã¨ã‚«ãƒ©ãƒ æƒ…å ±ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚’DuckDBã§å®Ÿè¡Œå¯èƒ½ãªSQLã‚¯ã‚¨ãƒªã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
SQLã‚¯ã‚¨ãƒªã®ã¿ã‚’ç”Ÿæˆã—ã€ä»–ã®èª¬æ˜æ–‡ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚

### ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©
{TABLE_SCHEMA}

### ã‚«ãƒ©ãƒ æƒ…å ±
{COLUMN_DEFINITIONS}

### åˆ©ç”¨å¯èƒ½ãªäº‹æ¥­ç¨®åˆ¥
{INDUSTRY_NAMES}

### åˆ©ç”¨å¯èƒ½ãªçŠ¯ç½ªåˆ†é¡(å¤§åˆ†é¡:å°åˆ†é¡)
{CRIMES_TYPES}

### å¯¾å¿œå¹´åº¦
2015å¹´ï½2024å¹´

### é‡è¦ãªãƒ«ãƒ¼ãƒ«
- SELECTæ–‡ã®ã¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼ˆINSERT/UPDATE/DELETEç¦æ­¢ï¼‰
- LIMITå¥ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§120ã‚’æŒ‡å®šï¼ˆæ˜ç¤ºçš„ã«æŒ‡å®šãŒã‚ã‚‹å ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆã™ã‚‹ï¼‰
- æ—¥æœ¬èªã®ã‚«ãƒ©ãƒ åã¯è‹±èªåã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: äº‹æ¥­æ‰€æ•° â†’ num_officesï¼‰

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{{user_question}}

### SQLã‚¯ã‚¨ãƒªï¼ˆSQLã®ã¿å‡ºåŠ›ã€èª¬æ˜ä¸è¦ï¼‰
"""

# --- ãƒ‡ãƒ¼ã‚¿é–¢é€£ ---

@st.cache_resource
def get_db_connection():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†åˆ©ç”¨"""
    try:
        return duckdb.connect('hachi_office.duckdb', read_only=True)
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_data
def load_geojson_data() -> Optional[gpd.GeoDataFrame]:
    """GeoJSONãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    try:
        gdf = gpd.read_file('geojson/hachiouji_aza_simplified.geojson')
        gdf = gdf[['S_NAME', 'geometry']].rename(columns={'S_NAME': 'town_name'})
        if gdf.crs is None:
            gdf = gdf.set_crs(epsg=4326)
        elif gdf.crs.to_epsg() != 4326:
            gdf = gdf.to_crs(epsg=4326)
        logger.info(f"GeoJSONèª­ã¿è¾¼ã¿æˆåŠŸ: {len(gdf)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        return gdf
    except Exception as e:
        logger.error(f"GeoJSONã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.error(f"GeoJSONã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def get_all_data(table_name: str) -> Optional[pd.DataFrame]:
    """æŒ‡å®šãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        con = get_db_connection()
        if con is None:
            return None
        return con.execute(f"SELECT * FROM {table_name}").fetchdf()
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ ({table_name}): {e}")
        return None

def execute_query(sql_query: str) -> Optional[pd.DataFrame]:
    """DuckDBã§SQLã‚’å®Ÿè¡Œã—ã€çµæœã‚’DataFrameã§è¿”ã™"""
    try:
        con = get_db_connection()
        if con is None:
            st.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        dangerous_keywords = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'ALTER', 'CREATE']
        if any(keyword in sql_query.upper() for keyword in dangerous_keywords):
            st.error("âš ï¸ å±é™ºãªSQLæ“ä½œãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            return None
        
        df = con.execute(sql_query).fetchdf()
        logger.info(f"ã‚¯ã‚¨ãƒªå®Ÿè¡ŒæˆåŠŸ: {len(df)}è¡Œå–å¾—")
        return df
    except Exception as e:
        st.error(f"âŒ SQLã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.error(f"SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}\nSQL: {sql_query}")
        return None

# --- AIé–¢é€£ ---

def generate_sql(question: str) -> Optional[str]:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‹ã‚‰SQLã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        model = genai.GenerativeModel('gemini-flash-latest')
        prompt = PROMPT_TEMPLATE.format(user_question=question)
        response = model.generate_content(prompt)
        sql_query = response.text.strip().replace("```sql", "").replace("```", "").strip()
        logger.info(f"ç”Ÿæˆã•ã‚ŒãŸSQL: {sql_query}")
        return sql_query
    except Exception as e:
        st.error(f"âŒ SQLã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.error(f"SQLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_query_parameters(sql_query: str, user_question: str) -> dict:
    """SQLã‚¯ã‚¨ãƒªã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã‹ã‚‰å¹´åº¦ãƒ»æ¥­ç¨®ãƒ»ç”ºåã‚’æŠ½å‡º"""
    params = {'year': None, 'industry': None, 'town': None}
    try:
        year_matches = re.findall(r'\b(20\d{2})\b', sql_query)
        if year_matches:
            params['year'] = int(year_matches[0])
        
        industry_keywords = INDUSTRY_NAMES.replace('\n', '').replace(' ', '').split(',')
        for industry in industry_keywords:
            if industry and (industry in sql_query or industry in user_question):
                params['industry'] = industry
                break
        
        town_match = re.search(r"town_name\s*=\s*'([^']+)'", sql_query)
        if town_match:
            params['town'] = town_match.group(1)
            
        logger.info(f"æŠ½å‡ºã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params}")
        return params
    except Exception as e:
        logger.error(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return params

def detect_metric_question(question: str) -> bool:
    """æŒ‡æ¨™è¨ˆç®—ãŒå¿…è¦ãªè³ªå•ã‹ã‚’åˆ¤å®š"""
    keywords = ['å¯†åº¦', 'æ¯”ç‡', 'å‰²åˆ', 'ä¸–å¸¯', 'äººå£', 'å¾“æ¥­è€…', 'ã‚ãŸã‚Š', 'æŒ‡æ¨™']
    return any(kw in question for kw in keywords)

# --- åˆ†æãƒ­ã‚¸ãƒƒã‚¯ ---

def calculate_derived_metrics(business_df: pd.DataFrame, population_df: pd.DataFrame, 
                              year: int = None, industry: str = None, town: str = None) -> Optional[pd.DataFrame]:
    """ä¸–å¸¯æ•°ã¨äº‹æ¥­æ‰€æ•°ã‹ã‚‰æ´¾ç”Ÿã—ãŸæŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹"""
    try:
        if business_df.empty or population_df.empty:
            return None
        
        filtered_business = business_df.copy()
        if year:
            filtered_business = filtered_business[filtered_business['year'] == year]
        if industry:
            filtered_business = filtered_business[filtered_business['industry_name'] == industry]
        if town:
            filtered_business = filtered_business[filtered_business['town_name'] == town]
        
        if filtered_business.empty:
            return None
        
        merged_df = pd.merge(
            filtered_business,
            population_df[['year', 'town_name', 'num_households', 'num_population']],
            on=['year', 'town_name'],
            how='inner'
        )

        if merged_df.empty:
            return None

        merged_df['office_density'] = merged_df['num_offices'] / merged_df['num_households'].replace(0, 1)
        merged_df['employee_ratio'] = merged_df['num_employees'] / merged_df['num_population'].replace(0, 1)
        merged_df['office_size'] = merged_df['num_employees'] / merged_df['num_offices'].replace(0, 1)
        merged_df['offices_per_1000_pop'] = (merged_df['num_offices'] / merged_df['num_population']) * 1000

        return merged_df
    except Exception as e:
        st.error(f"âŒ æŒ‡æ¨™ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.error(f"æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_interpretation(metrics_df: pd.DataFrame) -> str:
    """æŒ‡æ¨™ã®è§£é‡ˆã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
    if metrics_df is None or metrics_df.empty:
        return "è§£é‡ˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
    try:
        avg_density = metrics_df['office_density'].mean()
        avg_ratio = metrics_df['employee_ratio'].mean()
        avg_size = metrics_df['office_size'].mean()
        comments = []
        if avg_density > 0.1:
            comments.append(f"ğŸ¢ äº‹æ¥­æ‰€å¯†åº¦ãŒé«˜æ°´æº–ï¼ˆ{avg_density:.3f}ï¼‰ã§ã€å•†æ¥­æ´»å‹•ãŒæ´»ç™ºã§ã™ã€‚")
        elif avg_density > 0.05:
            comments.append(f"ğŸ“Š äº‹æ¥­æ‰€å¯†åº¦ã¯æ¨™æº–çš„ï¼ˆ{avg_density:.3f}ï¼‰ã§ã™ã€‚")
        else:
            comments.append(f"ğŸ˜ï¸ äº‹æ¥­æ‰€å¯†åº¦ãŒä½ã‚ï¼ˆ{avg_density:.3f}ï¼‰ã§ã€ä½å®…åœ°ä¸­å¿ƒã®ã‚¨ãƒªã‚¢ã§ã™ã€‚")
        if avg_ratio > 0.3:
            comments.append(f"ğŸ’¼ å¾“æ¥­è€…æ¯”ç‡ãŒé«˜ãï¼ˆ{avg_ratio:.3f}ï¼‰ã€é›‡ç”¨ãŒæ´»ç™ºã§ã™ã€‚")
        elif avg_ratio > 0.2:
            comments.append(f"ğŸ‘” å¾“æ¥­è€…æ¯”ç‡ã¯æ¨™æº–çš„ï¼ˆ{avg_ratio:.3f}ï¼‰ã§ã™ã€‚")
        else:
            comments.append(f"ğŸ  å¾“æ¥­è€…æ¯”ç‡ãŒä½ã‚ï¼ˆ{avg_ratio:.3f}ï¼‰ã§ã™ã€‚")
        if avg_size > 10:
            comments.append(f"ğŸ­ å¹³å‡äº‹æ¥­æ‰€è¦æ¨¡ãŒå¤§ããï¼ˆ{avg_size:.1f}äºº/æ‰€ï¼‰ã€ä¸­è¦æ¨¡ä»¥ä¸Šã®ä¼æ¥­ãŒå¤šã„ã§ã™ã€‚")
        else:
            comments.append(f"ğŸª å¹³å‡äº‹æ¥­æ‰€è¦æ¨¡ã¯å°ã•ã‚ï¼ˆ{avg_size:.1f}äºº/æ‰€ï¼‰ã§ã€å°è¦æ¨¡äº‹æ¥­æ‰€ä¸­å¿ƒã§ã™ã€‚")
        return " ".join(comments)
    except Exception as e:
        logger.error(f"è§£é‡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "è§£é‡ˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

def generate_contextual_explanation(user_question: str, metrics_df: pd.DataFrame) -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¿œã˜ãŸæ–‡è„ˆèª¬æ˜ã‚’ç”Ÿæˆ"""
    try:
        question_lower = user_question.lower()
        has_town = 'town_name' in metrics_df.columns and len(metrics_df['town_name'].unique()) > 1
        has_year = 'year' in metrics_df.columns and len(metrics_df['year'].unique()) > 1
        explanations = []
        if 'å¯†åº¦' in question_lower or 'ä¸–å¸¯' in question_lower:
            explanations.append("ã”è³ªå•ã®å†…å®¹ã«é–¢é€£ã—ã¦ã€**äº‹æ¥­æ‰€å¯†åº¦**ï¼ˆä¸–å¸¯æ•°ã«å¯¾ã™ã‚‹äº‹æ¥­æ‰€æ•°ã®æ¯”ç‡ï¼‰ã‚’åˆ†æã—ã¾ã—ãŸã€‚")
        if 'æ¯”ç‡' in question_lower or 'å‰²åˆ' in question_lower or 'äººå£' in question_lower:
            explanations.append("**å¾“æ¥­è€…æ¯”ç‡**ï¼ˆäººå£ã«å¯¾ã™ã‚‹å¾“æ¥­è€…æ•°ã®å‰²åˆï¼‰ã‚‚è¨ˆç®—ã—ã€åœ°åŸŸã®çµŒæ¸ˆæ´»å‹•ã®æ´»ç™ºã•ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚")
        if 'è¦æ¨¡' in question_lower or 'å¾“æ¥­è€…' in question_lower:
            explanations.append("**äº‹æ¥­æ‰€è¦æ¨¡**ï¼ˆ1äº‹æ¥­æ‰€ã‚ãŸã‚Šã®å¾“æ¥­è€…æ•°ï¼‰ã‹ã‚‰ã€äº‹æ¥­è€…ã®è¦æ¨¡æ„Ÿã‚’æŠŠæ¡ã§ãã¾ã™ã€‚")
        if has_town and has_year:
            explanations.append(f"\nğŸ“ {len(metrics_df['town_name'].unique())}ã®ç”ºåã€{len(metrics_df['year'].unique())}å¹´åº¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ã¦ã„ã¾ã™ã€‚")
        elif has_town:
            explanations.append(f"\nğŸ“ {len(metrics_df['town_name'].unique())}ã®ç”ºåã‚’æ¯”è¼ƒã—ã¦ã„ã¾ã™ã€‚")
        elif has_year:
            explanations.append(f"\nğŸ“… {len(metrics_df['year'].unique())}å¹´åº¦ã®æ¨ç§»ã‚’åˆ†æã—ã¦ã„ã¾ã™ã€‚")
        if not explanations:
            return "ã”è³ªå•ã«é–¢é€£ã™ã‚‹çµŒæ¸ˆæŒ‡æ¨™ã‚’è‡ªå‹•çš„ã«è¨ˆç®—ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®æŒ‡æ¨™ã§åœ°åŸŸã®ç‰¹å¾´ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚"
        return " ".join(explanations)
    except Exception as e:
        logger.error(f"æ–‡è„ˆèª¬æ˜ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "è³ªå•å†…å®¹ã«é–¢é€£ã™ã‚‹è¿½åŠ æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¾ã—ãŸã€‚"

def get_top_bottom_insights(metrics_df: pd.DataFrame, metric_name: str, display_name: str, n: int = 3) -> str:
    """æŒ‡æ¨™ã®ãƒˆãƒƒãƒ—ãƒ»ãƒœãƒˆãƒ ã‚’æŠ½å‡ºã—ã¦æ´å¯Ÿã‚’æä¾›"""
    try:
        if metric_name not in metrics_df.columns or 'town_name' not in metrics_df.columns:
            return ""
        df_sorted = metrics_df.sort_values(metric_name, ascending=False)
        if 'year' in df_sorted.columns:
            latest_year = df_sorted['year'].max()
            df_sorted = df_sorted[df_sorted['year'] == latest_year]
        top_towns = df_sorted.head(n)
        bottom_towns = df_sorted.tail(n)
        insights = f"\n\n**{display_name}ã®åœ°åŸŸå·®:**\n"
        insights += f"- ğŸ“ˆ **ä¸Šä½**: {', '.join([f'{row.town_name}ï¼ˆ{row[metric_name]:.3f}ï¼‰' for _, row in top_towns.iterrows()])}\n"
        insights += f"- ğŸ“‰ **ä¸‹ä½**: {', '.join([f'{row.town_name}ï¼ˆ{row[metric_name]:.3f}ï¼‰' for _, row in bottom_towns.iterrows()])}"
        return insights
    except Exception as e:
        logger.error(f"æ´å¯Ÿç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return ""


def generate_ai_summary(df: pd.DataFrame, user_question: str) -> str:
    """åˆ†æçµæœã‚’ã‚‚ã¨ã«GeminiãŒè‡ªç„¶è¨€èªã§å‚¾å‘ã‚’èª¬æ˜ã™ã‚‹"""
    if df is None or df.empty:
        return "ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    try:
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        model = genai.GenerativeModel('gemini-flash-latest')

        # DataFrameã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«å¤‰æ›ï¼ˆæœ€å¤§30è¡Œï¼‰
        sample_data = df.head(30).to_dict(orient="records")
        data_str = json.dumps(sample_data, ensure_ascii=False)

        prompt = f"""
æ¬¡ã®è³ªå•ã¨ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€å…«ç‹å­å¸‚ã«é–¢ã™ã‚‹åˆ†æçµæœã‚’æ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
è³ªå•: {user_question}
ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«: {data_str}

å‡ºåŠ›æ¡ä»¶:
- ä¸€èˆ¬åˆ©ç”¨è€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãã€2ã€œ4æ–‡ã§è¦ç´„ã€‚
- ä¸»ãªå‚¾å‘ãƒ»ç‰¹å¾´ãƒ»æ³¨ç›®ã™ã¹ãç‚¹ã‚’è¿°ã¹ã‚‹ã€‚
- æ•°å€¤ã‚„ç”ºåãŒæ˜ç¢ºãªå ´åˆã¯ãã‚Œã‚’æŒ™ã’ã‚‹ã€‚
"""

        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        logger.error(f"AIè¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "AIã«ã‚ˆã‚‹åˆ†æã‚³ãƒ¡ãƒ³ãƒˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
