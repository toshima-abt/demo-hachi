import streamlit as st
import duckdb
import pandas as pd
import google.generativeai as genai
from typing import Optional, Tuple
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- åˆæœŸè¨­å®š ---
st.set_page_config(page_title="å…«ç‹å­å¸‚ äº‹æ¥­è€…æ•°åˆ†æ", layout="wide")
st.title("ğŸ¢ è‡ªç„¶è¨€èªã§å…«ç‹å­å¸‚ã®äº‹æ¥­è€…ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ")

with st.expander("ğŸ“˜ ä½¿ã„æ–¹ã¨ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦"):
    st.markdown("""
    ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€å…«ç‹å­å¸‚ã®äº‹æ¥­è€…ã«é–¢ã™ã‚‹çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã€è‡ªç„¶è¨€èªã§è³ªå•ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    AIãŒã‚ãªãŸã®è³ªå•ã‚’è§£é‡ˆã—ã¦SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰çµæœã‚’å–å¾—ãƒ»è¡¨ç¤ºã—ã¾ã™ã€‚

    **åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿**
    - **äº‹æ¥­è€…çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ (`business_stats`)**:
        - **å¯¾å¿œå¹´åº¦**: 2015å¹´ï½2024å¹´
        - **äº‹æ¥­ç¨®åˆ¥**: è¾²æ—æ¼æ¥­, å»ºè¨­æ¥­, è£½é€ æ¥­, æƒ…å ±é€šä¿¡æ¥­, å¸å£²æ¥­_å°å£²æ¥­, å®¿æ³Šæ¥­_é£²é£Ÿã‚µãƒ¼ãƒ“ã‚¹æ¥­, åŒ»ç™‚_ç¦ç¥‰ãªã©18æ¥­ç¨®
        - **ã‚«ãƒ©ãƒ **: å¹´åº¦ã€ç”ºåã€äº‹æ¥­ç¨®åˆ¥ã€äº‹æ¥­æ‰€æ•°ã€å¾“æ¥­è€…æ•°
    - **äººå£çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ (`population`)**:
        - **ã‚«ãƒ©ãƒ **: å¹´åº¦ã€ç”ºåã€ä¸–å¸¯æ•°ã€äººå£æ•°ã€ç”·æ€§æ•°ã€å¥³æ€§æ•°

    **è³ªå•ã®ä¾‹**
    - `2021å¹´ã®ç”ºååˆ¥ã§ã€å»ºè¨­æ¥­ã®äº‹æ¥­æ‰€æ•°ãŒå¤šã„ãƒˆãƒƒãƒ—5ã‚’æ•™ãˆã¦`
    - `æƒ…å ±é€šä¿¡æ¥­ã®äº‹æ¥­æ‰€æ•°ãŒæœ€ã‚‚å¤šã„å¹´åº¦ã¯ï¼Ÿ`
    - `å…«ç‹å­å¸‚å…¨ä½“ã®å¾“æ¥­å“¡æ•°ã®æ¨ç§»ã‚’å¹´åº¦åˆ¥ã«æ•™ãˆã¦`
    - `2022å¹´ã®äººå£ã«å¯¾ã™ã‚‹äº‹æ¥­è€…æ•°ã®å‰²åˆãŒé«˜ã„ç”ºã¯ã©ã“ï¼Ÿ`

    **ã”æ³¨æ„**
    - AIãŒç”Ÿæˆã™ã‚‹SQLã¯å¿…ãšã—ã‚‚æ­£ç¢ºã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ„å›³ã—ãŸé€šã‚Šã®çµæœãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯ã€è³ªå•ã®ä»•æ–¹ã‚’å¤‰ãˆã¦ã¿ã¦ãã ã•ã„ã€‚
    """)

# --- å®šæ•°å®šç¾© ---
TABLE_SCHEMA = """
CREATE TABLE business_stats("year" INTEGER, town_name VARCHAR, industry_name VARCHAR, num_offices INTEGER, num_employees INTEGER);
CREATE TABLE population("year" BIGINT, town_name VARCHAR, num_households BIGINT, num_population BIGINT, num_male BIGINT, num_female BIGINT);
"""

COLUMN_DEFINITIONS = """
year: å¹´åº¦
town_name: ç”ºå
industry_name: äº‹æ¥­ç¨®åˆ¥
num_offices: äº‹æ¥­æ‰€æ•°
num_employees: å¾“æ¥­è€…æ•°
num_households: ä¸–å¸¯æ•°
num_population: äººå£æ•°
num_male: ç”·æ€§æ•°
num_female: å¥³æ€§æ•°
"""

INDUSTRY_NAMES = """
è¾²æ—æ¼æ¥­, é‰±æ¥­_æ¡çŸ³æ¥­_ç ‚åˆ©æ¡å–æ¥­, å»ºè¨­æ¥­, è£½é€ æ¥­, é›»æ°—ï½¥ã‚¬ã‚¹ï½¥ç†±ä¾›çµ¦ï½¥æ°´é“æ¥­, 
æƒ…å ±é€šä¿¡æ¥­, é‹è¼¸æ¥­_éƒµä¾¿æ¥­, å¸å£²æ¥­_å°å£²æ¥­, é‡‘èæ¥­_ä¿é™ºæ¥­, ä¸å‹•ç”£æ¥­_ç‰©å“è³ƒè²¸æ¥­, 
å­¦è¡“ç ”ç©¶_å°‚é–€ï½¥æŠ€è¡“ã‚µãƒ¼ãƒ“ã‚¹æ¥­, å®¿æ³Šæ¥­_é£²é£Ÿã‚µãƒ¼ãƒ“ã‚¹æ¥­, ç”Ÿæ´»é–¢é€£ã‚µãƒ¼ãƒ“ã‚¹æ¥­_å¨¯æ¥½æ¥­, 
æ•™è‚²_å­¦ç¿’æ”¯æ´æ¥­, åŒ»ç™‚_ç¦ç¥‰, è¤‡åˆã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­, ã‚µãƒ¼ãƒ“ã‚¹æ¥­ï¼ˆä»–ã«åˆ†é¡ã•ã‚Œãªã„ã‚‚ã®ï¼‰, 
å…¬å‹™ï¼ˆä»–ã«åˆ†é¡ã•ã‚Œã‚‹ã‚‚ã®ã‚’é™¤ãï¼‰
"""

PROMPT_TEMPLATE = f"""
ã‚ãªãŸã¯å„ªç§€ãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚å…«ç‹å­å¸‚ã«é–¢ã™ã‚‹ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã¨ã‚«ãƒ©ãƒ æƒ…å ±ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚’DuckDBã§å®Ÿè¡Œå¯èƒ½ãªSQLã‚¯ã‚¨ãƒªã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
SQLã‚¯ã‚¨ãƒªã®ã¿ã‚’ç”Ÿæˆã—ã€ä»–ã®èª¬æ˜æ–‡ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚

### ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©
{TABLE_SCHEMA}

### ã‚«ãƒ©ãƒ æƒ…å ±
{COLUMN_DEFINITIONS}

### åˆ©ç”¨å¯èƒ½ãªäº‹æ¥­ç¨®åˆ¥
{INDUSTRY_NAMES}

### å¯¾å¿œå¹´åº¦
2015å¹´ï½2024å¹´

### é‡è¦ãªãƒ«ãƒ¼ãƒ«
- SELECTæ–‡ã®ã¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼ˆINSERT/UPDATE/DELETEç¦æ­¢ï¼‰
- LIMITå¥ã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆãƒˆãƒƒãƒ—5ãªã‚‰ LIMIT 5ï¼‰
- æ—¥æœ¬èªã®ã‚«ãƒ©ãƒ åã¯è‹±èªåã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: äº‹æ¥­æ‰€æ•° â†’ num_officesï¼‰

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{{user_question}}

### SQLã‚¯ã‚¨ãƒªï¼ˆSQLã®ã¿å‡ºåŠ›ã€èª¬æ˜ä¸è¦ï¼‰
"""

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ã®æ¥ç¶šç®¡ç† ---
@st.cache_resource
def get_db_connection():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†åˆ©ç”¨"""
    try:
        return duckdb.connect('hachi_office.duckdb', read_only=True)
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- APIè¨­å®š ---
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
except Exception as e:
    st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlitã®Secretsã« 'GOOGLE_API_KEY' ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    logger.error(f"APIè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

# --- é–¢æ•°å®šç¾© ---
def generate_sql(question: str) -> Optional[str]:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‹ã‚‰SQLã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        prompt = PROMPT_TEMPLATE.format(user_question=question)
        response = model.generate_content(prompt)
        sql_query = response.text.strip().replace("```sql", "").replace("```", "").strip()
        logger.info(f"ç”Ÿæˆã•ã‚ŒãŸSQL: {sql_query}")
        return sql_query
    except Exception as e:
        st.error(f"âŒ SQLã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.error(f"SQLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

def execute_query(sql_query: str) -> Optional[pd.DataFrame]:
    """DuckDBã§SQLã‚’å®Ÿè¡Œã—ã€çµæœã‚’DataFrameã§è¿”ã™"""
    try:
        con = get_db_connection()
        if con is None:
            st.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬çš„ãªSQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰
        dangerous_keywords = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'ALTER', 'CREATE']
        if any(keyword in sql_query.upper() for keyword in dangerous_keywords):
            st.error("âš ï¸ å±é™ºãªSQLæ“ä½œãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            return None
        
        df = con.execute(sql_query).fetchdf()
        logger.info(f"ã‚¯ã‚¨ãƒªå®Ÿè¡ŒæˆåŠŸ: {len(df)}è¡Œå–å¾—")
        return df
    except Exception as e:
        st.error(f"âŒ SQLã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.error(f"SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}\nSQL: {sql_query}")
        return None

def calculate_derived_metrics(business_df: pd.DataFrame, population_df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """ä¸–å¸¯æ•°ã¨äº‹æ¥­æ‰€æ•°ã‹ã‚‰æ´¾ç”Ÿã—ãŸæŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹"""
    try:
        if business_df.empty or population_df.empty:
            return None
        
        # ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
        merged_df = pd.merge(
            business_df,
            population_df[['year', 'town_name', 'num_households', 'num_population']],
            on=['year', 'town_name'],
            how='inner'
        )

        if merged_df.empty:
            return None

        # æ´¾ç”ŸæŒ‡æ¨™ã®è¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—å¯¾ç­–ï¼‰
        merged_df['office_density'] = merged_df['num_offices'] / merged_df['num_households'].replace(0, 1)
        merged_df['employee_ratio'] = merged_df['num_employees'] / merged_df['num_population'].replace(0, 1)
        merged_df['office_size'] = merged_df['num_employees'] / merged_df['num_offices'].replace(0, 1)
        merged_df['offices_per_1000_pop'] = (merged_df['num_offices'] / merged_df['num_population']) * 1000

        logger.info(f"æ´¾ç”ŸæŒ‡æ¨™è¨ˆç®—å®Œäº†: {len(merged_df)}è¡Œ")
        return merged_df

    except Exception as e:
        st.error(f"âŒ æŒ‡æ¨™ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.error(f"æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_interpretation(metrics_df: pd.DataFrame) -> str:
    """æŒ‡æ¨™ã®è§£é‡ˆã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        if metrics_df is None or metrics_df.empty:
            return "è§£é‡ˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"

        avg_density = metrics_df['office_density'].mean()
        avg_ratio = metrics_df['employee_ratio'].mean()
        avg_size = metrics_df['office_size'].mean()

        comments = []

        # äº‹æ¥­æ‰€å¯†åº¦ã®è©•ä¾¡
        if avg_density > 0.1:
            comments.append(f"ğŸ¢ äº‹æ¥­æ‰€å¯†åº¦ãŒé«˜æ°´æº–ï¼ˆ{avg_density:.3f}ï¼‰ã§ã€å•†æ¥­æ´»å‹•ãŒæ´»ç™ºã§ã™ã€‚")
        elif avg_density > 0.05:
            comments.append(f"ğŸ“Š äº‹æ¥­æ‰€å¯†åº¦ã¯æ¨™æº–çš„ï¼ˆ{avg_density:.3f}ï¼‰ã§ã™ã€‚")
        else:
            comments.append(f"ğŸ˜ï¸ äº‹æ¥­æ‰€å¯†åº¦ãŒä½ã‚ï¼ˆ{avg_density:.3f}ï¼‰ã§ã€ä½å®…åœ°ä¸­å¿ƒã®ã‚¨ãƒªã‚¢ã§ã™ã€‚")

        # å¾“æ¥­è€…æ¯”ç‡ã®è©•ä¾¡
        if avg_ratio > 0.3:
            comments.append(f"ğŸ’¼ å¾“æ¥­è€…æ¯”ç‡ãŒé«˜ãï¼ˆ{avg_ratio:.3f}ï¼‰ã€é›‡ç”¨ãŒæ´»ç™ºã§ã™ã€‚")
        elif avg_ratio > 0.2:
            comments.append(f"ğŸ‘” å¾“æ¥­è€…æ¯”ç‡ã¯æ¨™æº–çš„ï¼ˆ{avg_ratio:.3f}ï¼‰ã§ã™ã€‚")
        else:
            comments.append(f"ğŸ  å¾“æ¥­è€…æ¯”ç‡ãŒä½ã‚ï¼ˆ{avg_ratio:.3f}ï¼‰ã§ã™ã€‚")

        # äº‹æ¥­æ‰€è¦æ¨¡ã®è©•ä¾¡
        if avg_size > 10:
            comments.append(f"ğŸ­ å¹³å‡äº‹æ¥­æ‰€è¦æ¨¡ãŒå¤§ããï¼ˆ{avg_size:.1f}äºº/æ‰€ï¼‰ã€ä¸­è¦æ¨¡ä»¥ä¸Šã®ä¼æ¥­ãŒå¤šã„ã§ã™ã€‚")
        else:
            comments.append(f"ğŸª å¹³å‡äº‹æ¥­æ‰€è¦æ¨¡ã¯å°ã•ã‚ï¼ˆ{avg_size:.1f}äºº/æ‰€ï¼‰ã§ã€å°è¦æ¨¡äº‹æ¥­æ‰€ä¸­å¿ƒã§ã™ã€‚")

        return " ".join(comments)

    except Exception as e:
        logger.error(f"è§£é‡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "è§£é‡ˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

def generate_contextual_explanation(user_question: str, metrics_df: pd.DataFrame) -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¿œã˜ãŸæ–‡è„ˆèª¬æ˜ã‚’ç”Ÿæˆ"""
    try:
        question_lower = user_question.lower()
        
        # ç”ºåãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        has_town = 'town_name' in metrics_df.columns and len(metrics_df['town_name'].unique()) > 1
        
        # å¹´åº¦ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        has_year = 'year' in metrics_df.columns and len(metrics_df['year'].unique()) > 1
        
        explanations = []
        
        # è³ªå•å†…å®¹ã«å¿œã˜ãŸèª¬æ˜
        if 'å¯†åº¦' in question_lower or 'ä¸–å¸¯' in question_lower:
            explanations.append("ã”è³ªå•ã®å†…å®¹ã«é–¢é€£ã—ã¦ã€**äº‹æ¥­æ‰€å¯†åº¦**ï¼ˆä¸–å¸¯æ•°ã«å¯¾ã™ã‚‹äº‹æ¥­æ‰€æ•°ã®æ¯”ç‡ï¼‰ã‚’åˆ†æã—ã¾ã—ãŸã€‚")
        
        if 'æ¯”ç‡' in question_lower or 'å‰²åˆ' in question_lower or 'äººå£' in question_lower:
            explanations.append("**å¾“æ¥­è€…æ¯”ç‡**ï¼ˆäººå£ã«å¯¾ã™ã‚‹å¾“æ¥­è€…æ•°ã®å‰²åˆï¼‰ã‚‚è¨ˆç®—ã—ã€åœ°åŸŸã®çµŒæ¸ˆæ´»å‹•ã®æ´»ç™ºã•ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚")
        
        if 'è¦æ¨¡' in question_lower or 'å¾“æ¥­è€…' in question_lower:
            explanations.append("**äº‹æ¥­æ‰€è¦æ¨¡**ï¼ˆ1äº‹æ¥­æ‰€ã‚ãŸã‚Šã®å¾“æ¥­è€…æ•°ï¼‰ã‹ã‚‰ã€äº‹æ¥­è€…ã®è¦æ¨¡æ„Ÿã‚’æŠŠæ¡ã§ãã¾ã™ã€‚")
        
        # æ¯”è¼ƒå¯¾è±¡ã®èª¬æ˜
        if has_town and has_year:
            explanations.append(f"\nğŸ“ {len(metrics_df['town_name'].unique())}ã®ç”ºåã€{len(metrics_df['year'].unique())}å¹´åº¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ã¦ã„ã¾ã™ã€‚")
        elif has_town:
            explanations.append(f"\nğŸ“ {len(metrics_df['town_name'].unique())}ã®ç”ºåã‚’æ¯”è¼ƒã—ã¦ã„ã¾ã™ã€‚")
        elif has_year:
            explanations.append(f"\nğŸ“… {len(metrics_df['year'].unique())}å¹´åº¦ã®æ¨ç§»ã‚’åˆ†æã—ã¦ã„ã¾ã™ã€‚")
        
        if not explanations:
            return "ã”è³ªå•ã«é–¢é€£ã™ã‚‹çµŒæ¸ˆæŒ‡æ¨™ã‚’è‡ªå‹•çš„ã«è¨ˆç®—ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®æŒ‡æ¨™ã§åœ°åŸŸã®ç‰¹å¾´ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚"
        
        return " ".join(explanations)
        
    except Exception as e:
        logger.error(f"æ–‡è„ˆèª¬æ˜ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "è³ªå•å†…å®¹ã«é–¢é€£ã™ã‚‹è¿½åŠ æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¾ã—ãŸã€‚"

def get_top_bottom_insights(metrics_df: pd.DataFrame, metric_name: str, display_name: str, n: int = 3) -> str:
    """æŒ‡æ¨™ã®ãƒˆãƒƒãƒ—ãƒ»ãƒœãƒˆãƒ ã‚’æŠ½å‡ºã—ã¦æ´å¯Ÿã‚’æä¾›"""
    try:
        if metric_name not in metrics_df.columns or 'town_name' not in metrics_df.columns:
            return ""
        
        df_sorted = metrics_df.sort_values(metric_name, ascending=False)
        
        # æœ€æ–°å¹´åº¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
        if 'year' in df_sorted.columns:
            latest_year = df_sorted['year'].max()
            df_sorted = df_sorted[df_sorted['year'] == latest_year]
        
        top_towns = df_sorted.head(n)
        bottom_towns = df_sorted.tail(n)
        
        insights = f"\n\n**{display_name}ã®åœ°åŸŸå·®:**\n"
        insights += f"- ğŸ“ˆ **ä¸Šä½**: {', '.join([f'{row.town_name}ï¼ˆ{row[metric_name]:.3f}ï¼‰' for _, row in top_towns.iterrows()])}\n"
        insights += f"- ğŸ“‰ **ä¸‹ä½**: {', '.join([f'{row.town_name}ï¼ˆ{row[metric_name]:.3f}ï¼‰' for _, row in bottom_towns.iterrows()])}"
        
        return insights
        
    except Exception as e:
        logger.error(f"æ´å¯Ÿç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return ""

def detect_metric_question(question: str) -> bool:
    """æŒ‡æ¨™è¨ˆç®—ãŒå¿…è¦ãªè³ªå•ã‹ã‚’åˆ¤å®š"""
    keywords = ['å¯†åº¦', 'æ¯”ç‡', 'å‰²åˆ', 'ä¸–å¸¯', 'äººå£', 'å¾“æ¥­è€…', 'ã‚ãŸã‚Š', 'æŒ‡æ¨™']
    return any(kw in question for kw in keywords)

def get_all_data(table_name: str) -> Optional[pd.DataFrame]:
    """æŒ‡å®šãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        con = get_db_connection()
        if con is None:
            return None
        return con.execute(f"SELECT * FROM {table_name}").fetchdf()
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ ({table_name}): {e}")
        return None

# --- UIéƒ¨åˆ† ---
st.markdown("---")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'user_question' not in st.session_state:
    st.session_state.user_question = "2021å¹´ã®äº‹æ¥­æ‰€å¯†åº¦ï¼ˆäº‹æ¥­æ‰€æ•°Ã·ä¸–å¸¯æ•°ï¼‰ã¨å¾“æ¥­è€…æ¯”ç‡ï¼ˆå¾“æ¥­è€…æ•°Ã·äººå£æ•°ï¼‰ã‚’ç”ºååˆ¥ã«æ¯”è¼ƒã—ã¦"
if 'result_df' not in st.session_state:
    st.session_state.result_df = None
if 'generated_sql' not in st.session_state:
    st.session_state.generated_sql = None
if 'metrics_df' not in st.session_state:
    st.session_state.metrics_df = None
if 'is_metric_question' not in st.session_state:
    st.session_state.is_metric_question = False

# ã‚µãƒ³ãƒ—ãƒ«è³ªå•ãƒœã‚¿ãƒ³
st.subheader("ğŸ’¡ è³ªå•ä¾‹")
col1, col2, col3 = st.columns(3)
with col1:
    if st.button("ğŸ—ï¸ å»ºè¨­æ¥­ãƒˆãƒƒãƒ—5"):
        st.session_state.user_question = "2021å¹´ã®å»ºè¨­æ¥­ã®äº‹æ¥­æ‰€æ•°ãŒå¤šã„ç”ºåãƒˆãƒƒãƒ—5"
with col2:
    if st.button("ğŸ“ˆ å¾“æ¥­å“¡æ•°æ¨ç§»"):
        st.session_state.user_question = "å¹´åº¦åˆ¥ã®å¾“æ¥­å“¡æ•°ã®æ¨ç§»"
with col3:
    if st.button("ğŸ˜ï¸ äº‹æ¥­æ‰€å¯†åº¦åˆ†æ"):
        st.session_state.user_question = "2022å¹´ã®ç”ºååˆ¥ã®äº‹æ¥­æ‰€å¯†åº¦ã‚’æ•™ãˆã¦"

# è³ªå•å…¥åŠ›ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã¨ç›´æ¥ãƒã‚¤ãƒ³ãƒ‰ï¼‰
user_question = st.text_input("ğŸ” åˆ†æã—ãŸã„å†…å®¹ã‚’è³ªå•ã—ã¦ãã ã•ã„:", key="user_question")

if st.button("ğŸš€ åˆ†æã‚’å®Ÿè¡Œ", type="primary"):
    if user_question:
        with st.spinner("ğŸ¤– AIãŒSQLã‚’ç”Ÿæˆä¸­..."):
            generated_sql = generate_sql(user_question)

        if generated_sql:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            st.session_state.generated_sql = generated_sql
            
            with st.spinner("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œä¸­..."):
                result_df = execute_query(generated_sql)
                st.session_state.result_df = result_df
                st.session_state.is_metric_question = detect_metric_question(user_question)

            # æ´¾ç”ŸæŒ‡æ¨™ã®è¨ˆç®—ï¼ˆæŒ‡æ¨™é–¢é€£ã®è³ªå•ã®å ´åˆï¼‰
            if st.session_state.is_metric_question and result_df is not None and not result_df.empty:
                with st.spinner("ğŸ“Š æ´¾ç”ŸæŒ‡æ¨™ã‚’è¨ˆç®—ä¸­..."):
                    population_df = get_all_data('population')
                    business_df = get_all_data('business_stats')

                    if population_df is not None and business_df is not None:
                        metrics_df = calculate_derived_metrics(business_df, population_df)
                        st.session_state.metrics_df = metrics_df
                    else:
                        st.session_state.metrics_df = None
            else:
                st.session_state.metrics_df = None
    else:
        st.warning("âš ï¸ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# --- çµæœè¡¨ç¤ºï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰å¾©å…ƒï¼‰ ---
if st.session_state.generated_sql:
    with st.expander("ğŸ“ ç”Ÿæˆã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª", expanded=False):
        st.code(st.session_state.generated_sql, language="sql")

if st.session_state.result_df is not None and not st.session_state.result_df.empty:
    result_df = st.session_state.result_df
    
    st.success(f"âœ… ã‚¯ã‚¨ãƒªçµæœ ({len(result_df)}è¡Œ)")
    st.dataframe(result_df, use_container_width=True)

    # --- æ´¾ç”ŸæŒ‡æ¨™ã®è¡¨ç¤º ---
    if st.session_state.is_metric_question and st.session_state.metrics_df is not None:
        metrics_df = st.session_state.metrics_df
        
        if not metrics_df.empty:
            st.markdown("---")
            
            # æ–‡è„ˆèª¬æ˜ã‚’è¡¨ç¤º
            context_explanation = generate_contextual_explanation(user_question, metrics_df)
            st.info(f"ğŸ” **åˆ†æã®èƒŒæ™¯**\n\n{context_explanation}")
            
            st.subheader("ğŸ“Š çµŒæ¸ˆæŒ‡æ¨™ã®è©³ç´°åˆ†æ")

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                avg_density = metrics_df['office_density'].mean()
                st.metric(
                    "å¹³å‡äº‹æ¥­æ‰€å¯†åº¦", 
                    f"{avg_density:.4f}", 
                    help="äº‹æ¥­æ‰€æ•° Ã· ä¸–å¸¯æ•°\n\nå€¤ãŒé«˜ã„ã»ã©ã€ä¸–å¸¯æ•°ã«å¯¾ã—ã¦äº‹æ¥­æ‰€ãŒå¤šã„ï¼ˆå•†æ¥­åœ°åŸŸçš„ï¼‰"
                )
            with col2:
                avg_ratio = metrics_df['employee_ratio'].mean()
                st.metric(
                    "å¹³å‡å¾“æ¥­è€…æ¯”ç‡", 
                    f"{avg_ratio:.4f}",
                    help="å¾“æ¥­è€…æ•° Ã· äººå£æ•°\n\nå€¤ãŒé«˜ã„ã»ã©ã€äººå£ã«å¯¾ã—ã¦åƒãäººãŒå¤šã„ï¼ˆé›‡ç”¨ãŒæ´»ç™ºï¼‰"
                )
            with col3:
                avg_size = metrics_df['office_size'].mean()
                st.metric(
                    "å¹³å‡äº‹æ¥­æ‰€è¦æ¨¡", 
                    f"{avg_size:.1f}äºº",
                    help="å¾“æ¥­è€…æ•° Ã· äº‹æ¥­æ‰€æ•°\n\nå€¤ãŒå¤§ãã„ã»ã©ã€1äº‹æ¥­æ‰€ã‚ãŸã‚Šã®å¾“æ¥­å“¡ãŒå¤šã„ï¼ˆå¤§è¦æ¨¡äº‹æ¥­æ‰€ï¼‰"
                )
            with col4:
                avg_per_1000 = metrics_df['offices_per_1000_pop'].mean()
                st.metric(
                    "äººå£1000äººã‚ãŸã‚Šäº‹æ¥­æ‰€æ•°", 
                    f"{avg_per_1000:.1f}",
                    help="(äº‹æ¥­æ‰€æ•° Ã· äººå£) Ã— 1000\n\nå›½éš›æ¯”è¼ƒãªã©ã§ä½¿ã‚ã‚Œã‚‹æ¨™æº–æŒ‡æ¨™"
                )

            # è§£é‡ˆã‚³ãƒ¡ãƒ³ãƒˆ
            interpretation = generate_interpretation(metrics_df)
            
            # åœ°åŸŸå·®ã®æ´å¯Ÿï¼ˆç”ºååˆ¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
            insights = ""
            if 'town_name' in metrics_df.columns and len(metrics_df['town_name'].unique()) > 1:
                insights += get_top_bottom_insights(metrics_df, 'office_density', 'äº‹æ¥­æ‰€å¯†åº¦', n=3)
            
            full_interpretation = f"ğŸ’¡ **ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ã“ã¨**\n\n{interpretation}{insights}"
            st.success(full_interpretation)

            # è©³ç´°ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
            with st.expander("ğŸ“‹ è©³ç´°ãªæŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º", expanded=False):
                display_cols = ['year', 'town_name', 'num_offices', 'num_employees', 'num_households', 'num_population',
                              'office_density', 'employee_ratio', 'office_size', 'offices_per_1000_pop']
                available_cols = [col for col in display_cols if col in metrics_df.columns]
                
                # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªåŒ–
                col_rename = {
                    'year': 'å¹´åº¦',
                    'town_name': 'ç”ºå',
                    'num_offices': 'äº‹æ¥­æ‰€æ•°',
                    'num_employees': 'å¾“æ¥­è€…æ•°',
                    'num_households': 'ä¸–å¸¯æ•°',
                    'num_population': 'äººå£æ•°',
                    'office_density': 'äº‹æ¥­æ‰€å¯†åº¦',
                    'employee_ratio': 'å¾“æ¥­è€…æ¯”ç‡',
                    'office_size': 'äº‹æ¥­æ‰€è¦æ¨¡',
                    'offices_per_1000_pop': 'äººå£åƒäººã‚ãŸã‚Šäº‹æ¥­æ‰€æ•°'
                }
                
                display_df = metrics_df[available_cols].copy()
                display_df = display_df.rename(columns=col_rename)
                
                st.dataframe(
                    display_df.round(4), 
                    use_container_width=True,
                    hide_index=True
                )
                
                # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                csv = display_df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    "ğŸ“¥ CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    csv,
                    "hachioji_metrics.csv",
                    "text/csv",
                    key='download-csv'
                )

    # --- ã‚°ãƒ©ãƒ•è¡¨ç¤º ---
    if len(result_df.columns) >= 2:
        try:
            numeric_cols = result_df.select_dtypes(include=['number']).columns.tolist()
            category_cols = result_df.select_dtypes(include=['object']).columns.tolist()

            if category_cols and numeric_cols:
                st.subheader("ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–")
                chart_df = result_df.set_index(category_cols[0])[numeric_cols[0]]
                st.bar_chart(chart_df)
        except Exception as e:
            logger.warning(f"ã‚°ãƒ©ãƒ•æç”»ã‚¹ã‚­ãƒƒãƒ—: {e}")
elif st.session_state.result_df is not None:
    st.warning("âš ï¸ çµæœãŒ0ä»¶ã§ã—ãŸã€‚è³ªå•ã‚’å¤‰ãˆã¦ã¿ã¦ãã ã•ã„ã€‚")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.caption("ğŸ’¡ Powered by Google Gemini & DuckDB | å…«ç‹å­å¸‚ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨")